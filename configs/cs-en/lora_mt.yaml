project:
  seed: 42
  output_dir: ../root/autodl-fs/outputs/mbart-wmt14-en-cs
  logging_dir: ../root/autodl-fs/runs/mbart-wmt14-en-cs
  cache_dir: ../root/autodl-fs/cache

data:
  dataset_name: wmt/wmt14
  dataset_config: cs-en
  source_language: en
  target_language: cs
  text_column: translation
  max_source_length: 256
  max_target_length: 256
  prompt_template: "translate {source_lang} to {target_lang}: {source_text}"
  max_train_samples: 100000000
  max_eval_samples: 1000

model:
  name: facebook/mbart-large-50-many-to-many-mmt
  tokenizer_src_lang_code: en_XX
  tokenizer_tgt_lang_code: cs_CZ
  gradient_checkpointing: true
  use_4bit: false
  torch_dtype: bfloat16

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  bias: none
  target_modules: ["q_proj", "v_proj", "k_proj", "out_proj", "fc1", "fc2"]
  task_type: seq2seq_lm

training:
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 4
  num_train_epochs: 3
  learning_rate: 0.0002
  weight_decay: 0.01
  warmup_ratio: 0.03
  lr_scheduler_type: cosine
  logging_steps: 10
  evaluation_strategy: steps
  eval_steps: 50
  save_strategy: steps
  save_steps: 2000
  save_total_limit: 3
  dataloader_num_workers: 4
  label_smoothing_factor: 0.1
  predict_with_generate: true
  generation_max_length: 256
  generation_num_beams: 4
  fp16: false
  bf16: true
  max_grad_norm: 1.0
  report_to: ["tensorboard"]
  load_best_model_at_end: true
  metric_for_best_model: bleu
  greater_is_better: true
  gradient_checkpointing: true
