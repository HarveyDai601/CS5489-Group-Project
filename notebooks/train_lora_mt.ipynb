{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97aeef01",
   "metadata": {},
   "source": [
    "# LoRA fine-tuning for WMT14\n",
    "æœ¬ Notebook å¤ç° `src/train_lora_mt.py` çš„è®­ç»ƒæµç¨‹ï¼Œå¹¶æŒ‰ç…§â€œç¯å¢ƒä¸ä¾èµ–å‡†å¤‡ â†’ æ•°æ®å¤„ç† â†’ æ¨¡å‹æ­å»º â†’ è®­ç»ƒ â†’ éªŒè¯ â†’ æ¨ç†ä¸ä¿å­˜â€çš„é¡ºåºç»„ç»‡ï¼Œä½¿å…¶æ›´é€‚åˆäº¤äº’å¼å®éªŒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7832e43",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒä¸ä¾èµ–å‡†å¤‡\n",
    "åœ¨æ­¤å•å…ƒä¸­å®‰è£…/æ ¡éªŒä¾èµ–ã€è®¾ç½®è·¯å¾„ä¸éšæœºç§å­ï¼Œå¹¶å‡†å¤‡ç¡¬ä»¶æ£€æµ‹ä¸å…¬ç”¨å·¥å…·å‡½æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ab7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¾æ®éœ€è¦å®‰è£…ä¾èµ–ï¼ˆè‹¥ç¯å¢ƒå·²æ»¡è¶³å¯è·³è¿‡ï¼‰\n",
    "!pip install -q transformers peft datasets accelerate evaluate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc61732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready. SRC_DIR= /Users/haoyudai/Desktop/CTU/CS5489 Machine Learning: Algorithms&Apns/Group Project/lora fintuing/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import inspect\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    " )\n",
    "import yaml\n",
    "\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    " )\n",
    "LOGGER = logging.getLogger(\"train_lora_mt_notebook\")\n",
    "\n",
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as handle:\n",
    "        return yaml.safe_load(handle)\n",
    "\n",
    "def get_dtype(dtype_name: str) -> torch.dtype:\n",
    "    lowered = dtype_name.lower()\n",
    "    if lowered in {\"bfloat16\", \"bf16\"}:\n",
    "        return torch.bfloat16\n",
    "    if lowered in {\"float16\", \"fp16\"}:\n",
    "        return torch.float16\n",
    "    return torch.float32\n",
    "\n",
    "def detect_device_type() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():  # type: ignore[attr-defined]\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "def prepare_training_arguments(project_cfg: Dict[str, Any], training_cfg: Dict[str, Any], output_dir: Path) -> Dict[str, Any]:\n",
    "    signature = inspect.signature(Seq2SeqTrainingArguments.__init__)\n",
    "    valid_params = set(signature.parameters.keys())\n",
    "    rename = {\"evaluation_strategy\": (\"evaluation_strategy\", \"eval_strategy\")}\n",
    "    args_dict: Dict[str, Any] = {}\n",
    "    for key, value in training_cfg.items():\n",
    "        dest = key\n",
    "        if key in rename:\n",
    "            for candidate in rename[key]:\n",
    "                if candidate in valid_params:\n",
    "                    dest = candidate\n",
    "                    break\n",
    "            else:\n",
    "                LOGGER.warning(\"Dropping unsupported training arg: %s\", key)\n",
    "                continue\n",
    "        if dest in valid_params:\n",
    "            args_dict[dest] = value\n",
    "        else:\n",
    "            LOGGER.warning(\"Dropping unsupported training arg: %s\", dest)\n",
    "    args_dict[\"output_dir\"] = str(output_dir)\n",
    "    if \"logging_dir\" in valid_params:\n",
    "        args_dict[\"logging_dir\"] = project_cfg.get(\"logging_dir\", \"runs\")\n",
    "    return args_dict\n",
    "\n",
    "def _require_grad_hook(_module, _inputs, output):\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        output.requires_grad_(True)\n",
    "\n",
    "def _ensure_embeddings_require_grad(target) -> None:\n",
    "    embeddings = getattr(target, \"get_input_embeddings\", lambda: None)()\n",
    "    if embeddings is not None and hasattr(embeddings, \"weight\"):\n",
    "        embeddings.weight.requires_grad_(True)\n",
    "\n",
    "def enable_gradient_checkpointing(model):\n",
    "    base_model = getattr(getattr(model, \"base_model\", None), \"model\", None) or getattr(model, \"model\", None) or model\n",
    "    base_model.gradient_checkpointing_enable()\n",
    "\n",
    "    activated = False\n",
    "    for target in (model, base_model):\n",
    "        if hasattr(target, \"enable_input_require_grads\"):\n",
    "            target.enable_input_require_grads()\n",
    "            activated = True\n",
    "    if not activated:\n",
    "        embeddings = getattr(base_model, \"get_input_embeddings\", lambda: None)()\n",
    "        if embeddings is not None:\n",
    "            embeddings.register_forward_hook(_require_grad_hook)\n",
    "\n",
    "    _ensure_embeddings_require_grad(base_model)\n",
    "\n",
    "    if hasattr(model, \"config\"):\n",
    "        model.config.use_cache = False\n",
    "\n",
    "def set_global_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "def load_translation_dataset(\n",
    "    dataset_name: str,\n",
    "    dataset_config: str | None = None,\n",
    "    cache_dir: str | None = None,\n",
    "    split_overrides: Dict[str, str] | None = None,\n",
    " ) -> DatasetDict:\n",
    "    if split_overrides:\n",
    "        dataset = DatasetDict()\n",
    "        for split_name, split_query in split_overrides.items():\n",
    "            dataset[split_name] = load_dataset(\n",
    "                dataset_name,\n",
    "                dataset_config,\n",
    "                split=split_query,\n",
    "                cache_dir=cache_dir,\n",
    "            )\n",
    "        return dataset\n",
    "    return load_dataset(dataset_name, dataset_config, cache_dir=cache_dir)\n",
    "\n",
    "def build_preprocess_function(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    data_cfg: Dict[str, Any],\n",
    " ) -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n",
    "    prompt_template = data_cfg[\"prompt_template\"]\n",
    "    source_language = data_cfg[\"source_language\"]\n",
    "    target_language = data_cfg[\"target_language\"]\n",
    "    text_column = data_cfg[\"text_column\"]\n",
    "    max_source_length = data_cfg[\"max_source_length\"]\n",
    "    max_target_length = data_cfg[\"max_target_length\"]\n",
    "\n",
    "    def preprocess_function(batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        translations = batch[text_column]\n",
    "        prompts, targets = [], []\n",
    "        for example in translations:\n",
    "            source_text = example[source_language]\n",
    "            target_text = example[target_language]\n",
    "            prompt = prompt_template.format(\n",
    "                source_lang=source_language,\n",
    "                target_lang=target_language,\n",
    "                source_text=source_text,\n",
    "            )\n",
    "            prompts.append(prompt)\n",
    "            targets.append(target_text)\n",
    "\n",
    "        model_inputs = tokenizer(prompts, max_length=max_source_length, truncation=True)\n",
    "        labels = tokenizer(text_target=targets, max_length=max_target_length, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    return preprocess_function\n",
    "\n",
    "def resolve_notebook_dir() -> Path:\n",
    "    if \"__file__\" in globals():\n",
    "        return Path(__file__).resolve().parent\n",
    "    cwd = Path.cwd()\n",
    "    if (cwd / \"train_lora_mt.ipynb\").exists():\n",
    "        return cwd\n",
    "    if (cwd / \"notebooks\" / \"train_lora_mt.ipynb\").exists():\n",
    "        return cwd / \"notebooks\"\n",
    "    return cwd\n",
    "\n",
    "NOTEBOOK_DIR = resolve_notebook_dir()\n",
    "print(\"Environment ready. NOTEBOOK_DIR=\", NOTEBOOK_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6fcd2",
   "metadata": {},
   "source": [
    "## 2. åŠ è½½ä¸é¢„å¤„ç†æ•°æ®é›†\n",
    "è¿™ä¸€éƒ¨åˆ†è¯»å– YAML é…ç½®ã€é€šè¿‡ ğŸ¤— Datasets æ‹‰å– WMT14 æ•°æ®ï¼Œæ„å»ºæ¸…æ´— + tokenize æµç¨‹ï¼Œå¹¶ç”¨ PyTorch DataLoader ç¼“å­˜åˆ†ç‰‡ä»¥ä¾¿å¤šä»»åŠ¡/å¤šè¿›ç¨‹åœºæ™¯å¤ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a45cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 19:45:11 - INFO - train_lora_mt_notebook - Detected backend: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1133d7e4d8940e9bd47493366054c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6dd1d92f95d4f81b455e52436c98f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192ea63cf4ac43828883ad93f74ad9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 19:45:18 - INFO - train_lora_mt_notebook - Loaded splits: train=train[:1000], eval=validation[:1000]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5805b3b4464bec9e54e67024af394b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5baf95a6b0964da4a14adf4b491debc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df18b9f47dcd4107b921a55e90a89620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad51805ed09d41cc8a7e7c3bba1273c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdc532b9ead45e1b85174501050075a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6add36f495b405b8ac9e92dd458dde3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch keys: KeysView({'input_ids': tensor([[250004,   3900,  19309,     22,     47,      8,     12,     87,    444,\n",
      "            959,  35778,  72219,    903,     74,    442,  96276,   1295,   1733,\n",
      "             47,   1733,    450,   3395,  25379,  22008,     47,  33636,   2856,\n",
      "              5,      2,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1],\n",
      "        [250004,   3900,  19309,     22,     47,      8,     12,   8414,  34202,\n",
      "              4,   1884,    759,  62975,     11,   6261,   8414, 122841,      4,\n",
      "            442,     83,     10,  17311, 147643,     47,      6,  58944,    136,\n",
      "           3249,    759,   5117, 116483,     47,    903,  13038,     98,    903,\n",
      "           4552,   5526,  31089,      4,  41866,  16792,     87,  33636,     10,\n",
      "           2831,    111,     70,  14098, 117604,      4,     70,  10542,  23166,\n",
      "          15534,      4,   3129,   1556,   1274,   9319,    188,  68073,    297,\n",
      "           1295, 134549,   5844,    116, 163157,     23,  17311,      5,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[250003,   1858,  54201,  20762,    654,    749,     74,    198,  15539,\n",
      "           3807,    960,   1248,      4,  44124,    332,    833, 151487,  21803,\n",
      "          87517,      5,      2,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
      "        [250003,  27420, 127486,      4,   2296,    443,    198, 102326,   1225,\n",
      "          39051, 140391,  27420,     19, 122841,    909,  47276,  86207,      4,\n",
      "          94756,      7,     23,   8186,  16384,    404,   8186,   3623, 148977,\n",
      "          15649,    381,  55961,    404,     72,  77415,      4,   2388,    289,\n",
      "            654,    491,    168,  10542,  23166,  15534,   1918,  16046,    224,\n",
      "         231715,     19,  89537,  31769,      7,  24732,  38417,      4,    122,\n",
      "          61710,     23,    168,  15937,  66966,    542,  24670,   5428,      9,\n",
      "         195602,    820,  31225,     19,  97873,    443,      5,      2]])})\n",
      "input_ids shape: torch.Size([2, 72])\n",
      "labels shape: torch.Size([2, 71])\n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = (NOTEBOOK_DIR / \"lora_mt_notebook.yaml\").resolve()\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Config not found at {CONFIG_PATH}\")\n",
    "OUTPUT_OVERRIDE = None  # å¯é€‰ï¼šæ‰‹åŠ¨è¦†ç›–è¾“å‡ºç›®å½•\n",
    "\n",
    "config = load_config(str(CONFIG_PATH))\n",
    "if OUTPUT_OVERRIDE:\n",
    "    config[\"project\"][\"output_dir\"] = str(OUTPUT_OVERRIDE)\n",
    "\n",
    "project_cfg = dict(config[\"project\"])\n",
    "data_cfg = dict(config[\"data\"])\n",
    "model_cfg = dict(config[\"model\"])\n",
    "lora_cfg = dict(config[\"lora\"])\n",
    "training_cfg = dict(config[\"training\"])  # åç»­å¯åœ¨ Notebook å†…åŠ¨æ€ä¿®æ”¹\n",
    "training_cfg.setdefault(\"dataloader_pin_memory\", True)\n",
    "\n",
    "set_global_seed(project_cfg.get(\"seed\", 42))\n",
    "device_type = detect_device_type()\n",
    "LOGGER.info(\"Detected backend: %s\", device_type)\n",
    "\n",
    "if model_cfg.get(\"use_4bit\") and device_type != \"cuda\":\n",
    "    LOGGER.warning(\"4-bit quantization requires CUDA; disabling.\")\n",
    "    model_cfg[\"use_4bit\"] = False\n",
    "\n",
    "def _split_query(base: str, max_key: str) -> str:\n",
    "    max_samples = data_cfg.get(max_key)\n",
    "    return f\"{base}[:{int(max_samples)}]\" if max_samples else base\n",
    "\n",
    "train_query = _split_query(\"train\", \"max_train_samples\")\n",
    "eval_base = \"validation\"\n",
    "eval_query = _split_query(eval_base, \"max_eval_samples\")\n",
    "\n",
    "try:\n",
    "    raw_datasets = load_translation_dataset(\n",
    "        data_cfg[\"dataset_name\"],\n",
    "        data_cfg.get(\"dataset_config\"),\n",
    "        cache_dir=project_cfg.get(\"cache_dir\"),\n",
    "        split_overrides={\"train\": train_query, eval_base: eval_query},\n",
    "    )\n",
    "    eval_split = eval_base\n",
    "except ValueError:\n",
    "    eval_base = \"test\"\n",
    "    eval_query = _split_query(eval_base, \"max_eval_samples\")\n",
    "    raw_datasets = load_translation_dataset(\n",
    "        data_cfg[\"dataset_name\"],\n",
    "        data_cfg.get(\"dataset_config\"),\n",
    "        cache_dir=project_cfg.get(\"cache_dir\"),\n",
    "        split_overrides={\"train\": train_query, eval_base: eval_query},\n",
    "    )\n",
    "    eval_split = eval_base\n",
    "\n",
    "LOGGER.info(\"Loaded splits: train=%s, eval=%s\", train_query, eval_query)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_cfg[\"name\"], cache_dir=project_cfg.get(\"cache_dir\"))\n",
    "tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "tokenizer.src_lang = model_cfg[\"tokenizer_src_lang_code\"]\n",
    "tokenizer.tgt_lang = model_cfg[\"tokenizer_tgt_lang_code\"]\n",
    "\n",
    "preprocess_fn = build_preprocess_function(tokenizer, data_cfg)\n",
    "remove_columns = raw_datasets[\"train\"].column_names\n",
    "processed_datasets = raw_datasets.map(\n",
    "    preprocess_fn,\n",
    "    batched=True,\n",
    "    remove_columns=remove_columns,\n",
    "    desc=\"Tokenizing dataset\",\n",
    " )\n",
    "\n",
    "preview_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=None)\n",
    "train_loader = DataLoader(\n",
    "    processed_datasets[\"train\"],\n",
    "    batch_size=training_cfg[\"per_device_train_batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=preview_collator,\n",
    " )\n",
    "\n",
    "first_batch = next(iter(train_loader))\n",
    "print(\"Sample batch keys:\", first_batch.keys())\n",
    "print(\"input_ids shape:\", first_batch[\"input_ids\"].shape)\n",
    "print(\"labels shape:\", first_batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344670c",
   "metadata": {},
   "source": [
    "## 3. æ„å»ºåŸºç¡€æ¨¡å‹ä¸ LoRA é€‚é…å™¨\n",
    "åŠ è½½ mBART ä¸»å¹²ã€å¯é€‰ 4-bit é‡åŒ–ï¼Œé…ç½® LoRA Rank/Alpha/Dropout å¹¶ä»…è®­ç»ƒç›®æ ‡æ¨¡å—ï¼Œæœ€åæ£€æŸ¥å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe779e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 19:45:27 - WARNING - train_lora_mt_notebook - mps backend detected; falling back to float32 weights.\n",
      "2025-11-28 19:45:27 - INFO - train_lora_mt_notebook - Disabling bf16 for mps backend.\n",
      "2025-11-28 19:45:27 - INFO - train_lora_mt_notebook - Disabling gradient_checkpointing for mps backend.\n",
      "2025-11-28 19:45:27 - INFO - train_lora_mt_notebook - Disabling bf16 for mps backend.\n",
      "2025-11-28 19:45:27 - INFO - train_lora_mt_notebook - Disabling gradient_checkpointing for mps backend.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf736dfc444c41a9827e0d7d055cdd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ffd055d81443f5b9358004652e7d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 2,359,296 || all params: 613,238,784 || trainable%: 0.3847\n",
      "Trainable params: 2.36M / 613.24M (0.38%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haoyudai/miniconda3/envs/ml/lib/python3.14/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "dtype = get_dtype(model_cfg.get(\"torch_dtype\", \"float32\"))\n",
    "if device_type != \"cuda\" and dtype in (torch.float16, torch.bfloat16):\n",
    "    LOGGER.warning(\"%s backend detected; falling back to float32 weights.\", device_type)\n",
    "    dtype = torch.float32\n",
    "\n",
    "if device_type != \"cuda\":\n",
    "    for flag in (\"bf16\", \"fp16\", \"gradient_checkpointing\"):\n",
    "        if training_cfg.get(flag):\n",
    "            LOGGER.info(\"Disabling %s for %s backend.\", flag, device_type)\n",
    "            training_cfg[flag] = False\n",
    "    if training_cfg.get(\"dataloader_num_workers\", 0) > 0:\n",
    "        training_cfg[\"dataloader_num_workers\"] = 0\n",
    "    training_cfg[\"dataloader_pin_memory\"] = False\n",
    "\n",
    "model_kwargs: Dict[str, Any] = {\"cache_dir\": project_cfg.get(\"cache_dir\")}\n",
    "if model_cfg.get(\"use_4bit\"):\n",
    "    quant_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "    model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=quant_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    model_kwargs[\"device_map\"] = \"auto\"\n",
    "else:\n",
    "    model_kwargs[\"torch_dtype\"] = dtype\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_cfg[\"name\"], **model_kwargs)\n",
    "if model_cfg.get(\"use_4bit\"):\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "if hasattr(model.config, \"forced_bos_token_id\") and hasattr(tokenizer, \"lang_code_to_id\"):\n",
    "    model.config.forced_bos_token_id = tokenizer.lang_code_to_id[model_cfg[\"tokenizer_tgt_lang_code\"]]\n",
    "\n",
    "task_type = TaskType.SEQ_2_SEQ_LM if lora_cfg[\"task_type\"].lower() == \"seq2seq_lm\" else TaskType.SEQ_CLS\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_cfg[\"r\"],\n",
    "    lora_alpha=lora_cfg[\"alpha\"],\n",
    "    target_modules=lora_cfg[\"target_modules\"],\n",
    "    lora_dropout=lora_cfg[\"dropout\"],\n",
    "    bias=lora_cfg.get(\"bias\", \"none\"),\n",
    "    task_type=task_type,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "if training_cfg.get(\"gradient_checkpointing\"):\n",
    "    enable_gradient_checkpointing(model)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable params: {trainable_params/1e6:.2f}M / {total_params/1e6:.2f}M ({100*trainable_params/total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c98e46e",
   "metadata": {},
   "source": [
    "## 4. å®ç° `train_lora_mt` è®­ç»ƒå¾ªç¯\n",
    "æ­¤å¤„å¤ç°è„šæœ¬è®­ç»ƒé€»è¾‘ï¼šè‡ªå®šä¹‰å¤šä»»åŠ¡æŸå¤±èšåˆ $\\mathcal{L}_{total}=\\sum_i w_i\\mathcal{L}_i$ï¼Œé…ç½®ä¼˜åŒ–å™¨/è°ƒåº¦å™¨ã€æ¢¯åº¦ç´¯ç§¯ä¸æ–­ç‚¹æ¢å¤ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2f042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 19:47:43 - INFO - train_lora_mt_notebook - Training arguments: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=True,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=False,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=50,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=256,\n",
      "generation_num_beams=4,\n",
      "gradient_accumulation_steps=8,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_revision=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=no,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.1,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "liger_kernel_config=None,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=runs/mbart-wmt14-en-de,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.COSINE,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=bleu,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=OptimizerNames.ADAMW_TORCH_FUSED,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=outputs/mbart-wmt14-en-de,\n",
      "overwrite_output_dir=False,\n",
      "parallelism_config=None,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "project=huggingface,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=None,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2000,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trackio_space_id=trackio,\n",
      "use_cpu=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n",
      "/var/folders/3f/ghm3dh2x3596fy8v46pyr4b00000gn/T/ipykernel_4851/630915525.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedSeq2SeqTrainer(\n",
      "/var/folders/3f/ghm3dh2x3596fy8v46pyr4b00000gn/T/ipykernel_4851/630915525.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedSeq2SeqTrainer(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "WeightedSeq2SeqTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     29\u001b[39m trainer = WeightedSeq2SeqTrainer(\n\u001b[32m     30\u001b[39m     model=model,\n\u001b[32m     31\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     data_collator=data_collator,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m resume_checkpoint = project_cfg.get(\u001b[33m\"\u001b[39m\u001b[33mresume_from_checkpoint\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m trainer.save_model()\n\u001b[32m     42\u001b[39m train_metrics = train_result.metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.14/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.14/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.14/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[31mTypeError\u001b[39m: WeightedSeq2SeqTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'"
     ]
    }
   ],
   "source": [
    "loss_weights = {\"translation\": 1.0}  # å¯æŒ‰ä»»åŠ¡ç»´åº¦è°ƒæ•´\n",
    "\n",
    "\n",
    "class WeightedSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    \"\"\"ç¤ºä¾‹ï¼šæŒ‰ task_id èšåˆå¤šä»»åŠ¡æŸå¤±ï¼Œå…¼å®¹ transformers>=4.45 compute_loss ç­¾åã€‚\"\"\"\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):  # type: ignore[override]\n",
    "        task_ids = inputs.pop(\"task_id\", None)\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        if task_ids is not None:\n",
    "            unique_ids, counts = torch.unique(task_ids, return_counts=True)\n",
    "            weighted_loss = torch.zeros_like(loss)\n",
    "            for task, count in zip(unique_ids.tolist(), counts.tolist()):\n",
    "                weight = loss_weights.get(task, 1.0)\n",
    "                weighted_loss = weighted_loss + weight * loss / max(count, 1)\n",
    "            loss = weighted_loss\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "\n",
    "try:\n",
    "    import evaluate\n",
    "\n",
    "    bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "except Exception as exc:  # pylint: disable=broad-except\n",
    "    LOGGER.warning(\"Falling back to eval_loss only because sacrebleu failed: %s\", exc)\n",
    "    bleu_metric = None\n",
    "    desired_metric = training_cfg.get(\"metric_for_best_model\")\n",
    "    if desired_metric and desired_metric != \"eval_loss\":\n",
    "        LOGGER.warning(\n",
    "            \"Overriding metric_for_best_model=%s to eval_loss because BLEU metrics are unavailable.\",\n",
    "            desired_metric,\n",
    "        )\n",
    "        training_cfg[\"metric_for_best_model\"] = \"eval_loss\"\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    if bleu_metric is None:\n",
    "        return {}\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    bleu = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    return {\"bleu\": bleu[\"score\"], \"gen_len\": float(np.mean(prediction_lens))}\n",
    "\n",
    "\n",
    "output_dir = Path(project_cfg[\"output_dir\"])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "training_args = Seq2SeqTrainingArguments(**prepare_training_arguments(project_cfg, training_cfg, output_dir))\n",
    "LOGGER.info(\"Training arguments: %s\", training_args)\n",
    "\n",
    "trainer = WeightedSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[eval_split],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "resume_checkpoint = project_cfg.get(\"resume_from_checkpoint\")\n",
    "train_result = trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "trainer.save_model()\n",
    "\n",
    "train_metrics = train_result.metrics\n",
    "train_metrics[\"train_samples\"] = len(processed_datasets[\"train\"])\n",
    "trainer.log_metrics(\"train\", train_metrics)\n",
    "trainer.save_metrics(\"train\", train_metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971f403",
   "metadata": {},
   "source": [
    "## 5. éªŒè¯ä¸æŒ‡æ ‡å¯è§†åŒ–\n",
    "å®ç° `evaluate()` è®¡ç®— BLEU/Perplexity/Accuracyï¼Œå¹¶æŠŠæ—¥å¿—å†™å…¥ pandasï¼Œå€ŸåŠ© matplotlib å¯è§†åŒ– loss ä¸å­¦ä¹ ç‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0d630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def evaluate_model(max_length: int | None = None, num_beams: int | None = None) -> Dict[str, float]:\n",
    "    eval_kwargs = {}\n",
    "    if max_length:\n",
    "        eval_kwargs[\"max_length\"] = max_length\n",
    "    if num_beams:\n",
    "        eval_kwargs[\"num_beams\"] = num_beams\n",
    "    eval_raw = trainer.evaluate(**eval_kwargs)\n",
    "\n",
    "    predicts = trainer.predict(processed_datasets[eval_split], **eval_kwargs)\n",
    "    decoded_preds = tokenizer.batch_decode(predicts.predictions, skip_special_tokens=True)\n",
    "    labels = np.where(predicts.label_ids != -100, predicts.label_ids, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    bleu = bleu_metric.compute(predictions=decoded_preds, references=[[lbl] for lbl in decoded_labels])\n",
    "    exact_match = np.mean([int(p == l) for p, l in zip(decoded_preds, decoded_labels)])\n",
    "    ppl = math.exp(min(eval_raw[\"eval_loss\"], 20))\n",
    "\n",
    "    metrics = {\n",
    "        **eval_raw,\n",
    "        \"bleu\": bleu[\"score\"],\n",
    "        \"exact_match\": float(exact_match),\n",
    "        \"perplexity\": float(ppl),\n",
    "    }\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "eval_metrics = evaluate_model(\n",
    "    max_length=training_cfg.get(\"generation_max_length\"),\n",
    "    num_beams=training_cfg.get(\"generation_num_beams\"),\n",
    ")\n",
    "print(\"Eval metrics:\", eval_metrics)\n",
    "\n",
    "# è®°å½•å­¦ä¹ ç‡ä¸æŸå¤±ï¼Œå¯è§†åŒ–\n",
    "log_history = pd.DataFrame(trainer.state.log_history)\n",
    "log_history = log_history.dropna(subset=[\"loss\"], how=\"all\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "log_history.plot(x=\"step\", y=\"loss\", ax=axes[0], title=\"Training loss\")\n",
    "if \"learning_rate\" in log_history:\n",
    "    log_history.plot(x=\"step\", y=\"learning_rate\", ax=axes[1], title=\"Learning rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc03c0",
   "metadata": {},
   "source": [
    "## 6. æ¨ç†æµ‹è¯•ä¸æƒé‡ä¿å­˜\n",
    "é€šè¿‡è‡ªå®šä¹‰ `generate_translation()` å¯¹æ ·ä¾‹æç¤ºåšæ¨ç†ï¼Œå¯é€‰å¯¹æ¯”æœª LoRA çš„ baselineï¼Œå¹¶åˆ†åˆ«ä¿å­˜ LoRA é€‚é…å™¨ä¸åˆå¹¶åçš„å…¨é‡æƒé‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49bbcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARE_BASELINE = False  # è‹¥ä¸º Trueï¼Œåˆ™é¢å¤–è½½å…¥æœª LoRA çš„ backbone è¿›è¡Œå¯¹æ¯”\n",
    "\n",
    "\n",
    "def generate_translation(text: str, model_to_use=None, tokenizer_to_use=None, max_new_tokens: int = 128) -> str:\n",
    "    model_to_use = model_to_use or trainer.model\n",
    "    tokenizer_to_use = tokenizer_to_use or tokenizer\n",
    "    prompt = data_cfg[\"prompt_template\"].format(\n",
    "        source_lang=data_cfg[\"source_language\"],\n",
    "        target_lang=data_cfg[\"target_language\"],\n",
    "        source_text=text,\n",
    "    )\n",
    "    inputs = tokenizer_to_use(prompt, return_tensors=\"pt\").to(model_to_use.device)\n",
    "    with torch.inference_mode():\n",
    "        generated = model_to_use.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=4)\n",
    "    return tokenizer_to_use.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "demo_text = \"The upcoming weekend weather looks great for hiking.\"\n",
    "print(\"LoRA output:\", generate_translation(demo_text))\n",
    "\n",
    "if COMPARE_BASELINE:\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_cfg[\"name\"],\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        cache_dir=project_cfg.get(\"cache_dir\"),\n",
    "    )\n",
    "    base_model.eval()\n",
    "    print(\"Baseline output:\", generate_translation(demo_text, model_to_use=base_model))\n",
    "\n",
    "# ä¿å­˜ LoRA é€‚é…å™¨\n",
    "lora_dir = output_dir / \"lora_adapter\"\n",
    "lora_dir.mkdir(exist_ok=True)\n",
    "trainer.model.save_pretrained(lora_dir)\n",
    "tokenizer.save_pretrained(lora_dir)\n",
    "print(\"LoRA adapter saved to\", lora_dir)\n",
    "\n",
    "# åˆå¹¶æƒé‡å¹¶å¯¼å‡ºï¼ˆç”¨äºçº¯æ¨ç†éƒ¨ç½²ï¼‰\n",
    "merged_dir = output_dir / \"merged_full_model\"\n",
    "merged_dir.mkdir(exist_ok=True)\n",
    "merged_model = trainer.model.merge_and_unload()\n",
    "merged_model.save_pretrained(merged_dir)\n",
    "tokenizer.save_pretrained(merged_dir)\n",
    "print(\"Merged model saved to\", merged_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
