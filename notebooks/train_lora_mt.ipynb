{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97aeef01",
   "metadata": {},
   "source": [
    "# LoRA fine-tuning for WMT14\n",
    "æœ¬ Notebook å¤ç° `src/train_lora_mt.py` çš„è®­ç»ƒæµç¨‹ï¼Œå¹¶æŒ‰ç…§â€œç¯å¢ƒä¸ä¾èµ–å‡†å¤‡ â†’ æ•°æ®å¤„ç† â†’ æ¨¡å‹æ­å»º â†’ è®­ç»ƒ â†’ éªŒè¯ â†’ æ¨ç†ä¸ä¿å­˜â€çš„é¡ºåºç»„ç»‡ï¼Œä½¿å…¶æ›´é€‚åˆäº¤äº’å¼å®éªŒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7832e43",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒä¸ä¾èµ–å‡†å¤‡\n",
    "åœ¨æ­¤å•å…ƒä¸­å®‰è£…/æ ¡éªŒä¾èµ–ã€è®¾ç½®è·¯å¾„ä¸éšæœºç§å­ï¼Œå¹¶å‡†å¤‡ç¡¬ä»¶æ£€æµ‹ä¸å…¬ç”¨å·¥å…·å‡½æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ab7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¾æ®éœ€è¦å®‰è£…ä¾èµ–ï¼ˆè‹¥ç¯å¢ƒå·²æ»¡è¶³å¯è·³è¿‡ï¼‰\n",
    "!pip install -q transformers peft datasets accelerate evaluate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc61732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import inspect\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    set_seed,\n",
    ")\n",
    "import yaml\n",
    "\n",
    "# å…è®¸ notebook ç›´æ¥å¤ç”¨é¡¹ç›®æ¨¡å—\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "import sys\n",
    "\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "from data_utils import build_preprocess_function, load_translation_dataset  # noqa: E402\n",
    "\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "LOGGER = logging.getLogger(\"train_lora_mt_notebook\")\n",
    "\n",
    "\n",
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as handle:\n",
    "        return yaml.safe_load(handle)\n",
    "\n",
    "\n",
    "def get_dtype(dtype_name: str) -> torch.dtype:\n",
    "    lowered = dtype_name.lower()\n",
    "    if lowered in {\"bfloat16\", \"bf16\"}:\n",
    "        return torch.bfloat16\n",
    "    if lowered in {\"float16\", \"fp16\"}:\n",
    "        return torch.float16\n",
    "    return torch.float32\n",
    "\n",
    "\n",
    "def detect_device_type() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():  # type: ignore[attr-defined]\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "\n",
    "def prepare_training_arguments(project_cfg: Dict[str, Any], training_cfg: Dict[str, Any], output_dir: Path) -> Dict[str, Any]:\n",
    "    signature = inspect.signature(Seq2SeqTrainingArguments.__init__)\n",
    "    valid_params = set(signature.parameters.keys())\n",
    "    rename = {\"evaluation_strategy\": (\"evaluation_strategy\", \"eval_strategy\")}\n",
    "    args_dict: Dict[str, Any] = {}\n",
    "    for key, value in training_cfg.items():\n",
    "        dest = key\n",
    "        if key in rename:\n",
    "            for candidate in rename[key]:\n",
    "                if candidate in valid_params:\n",
    "                    dest = candidate\n",
    "                    break\n",
    "            else:\n",
    "                LOGGER.warning(\"Dropping unsupported training arg: %s\", key)\n",
    "                continue\n",
    "        if dest in valid_params:\n",
    "            args_dict[dest] = value\n",
    "        else:\n",
    "            LOGGER.warning(\"Dropping unsupported training arg: %s\", dest)\n",
    "    args_dict[\"output_dir\"] = str(output_dir)\n",
    "    if \"logging_dir\" in valid_params:\n",
    "        args_dict[\"logging_dir\"] = project_cfg.get(\"logging_dir\", \"runs\")\n",
    "    return args_dict\n",
    "\n",
    "\n",
    "def enable_gradient_checkpointing(model):\n",
    "    base_model = getattr(getattr(model, \"base_model\", None), \"model\", model)\n",
    "    base_model.gradient_checkpointing_enable()\n",
    "    if hasattr(base_model, \"enable_input_require_grads\"):\n",
    "        base_model.enable_input_require_grads()\n",
    "    else:\n",
    "        input_embeddings = base_model.get_input_embeddings()\n",
    "        if input_embeddings is not None:\n",
    "            def _require_grad(_, __, output):\n",
    "                if isinstance(output, torch.Tensor):\n",
    "                    output.requires_grad_(True)\n",
    "\n",
    "            input_embeddings.register_forward_hook(_require_grad)\n",
    "    if hasattr(model, \"config\"):\n",
    "        model.config.use_cache = False\n",
    "\n",
    "\n",
    "def set_global_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "\n",
    "print(\"Environment ready. SRC_DIR=\", SRC_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6fcd2",
   "metadata": {},
   "source": [
    "## 2. åŠ è½½ä¸é¢„å¤„ç†æ•°æ®é›†\n",
    "è¿™ä¸€éƒ¨åˆ†è¯»å– YAML é…ç½®ã€é€šè¿‡ ğŸ¤— Datasets æ‹‰å– WMT14 æ•°æ®ï¼Œæ„å»ºæ¸…æ´— + tokenize æµç¨‹ï¼Œå¹¶ç”¨ PyTorch DataLoader ç¼“å­˜åˆ†ç‰‡ä»¥ä¾¿å¤šä»»åŠ¡/å¤šè¿›ç¨‹åœºæ™¯å¤ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a45cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = PROJECT_ROOT / \"configs\" / \"lora_mt.yaml\"\n",
    "OUTPUT_OVERRIDE = None  # å¯é€‰ï¼šæ‰‹åŠ¨è¦†ç›–è¾“å‡ºç›®å½•\n",
    "\n",
    "config = load_config(str(CONFIG_PATH))\n",
    "if OUTPUT_OVERRIDE:\n",
    "    config[\"project\"][\"output_dir\"] = str(OUTPUT_OVERRIDE)\n",
    "\n",
    "project_cfg = dict(config[\"project\"])\n",
    "data_cfg = dict(config[\"data\"])\n",
    "model_cfg = dict(config[\"model\"])\n",
    "lora_cfg = dict(config[\"lora\"])\n",
    "training_cfg = dict(config[\"training\"])  # åç»­å¯åœ¨ Notebook å†…åŠ¨æ€ä¿®æ”¹\n",
    "training_cfg.setdefault(\"dataloader_pin_memory\", True)\n",
    "\n",
    "set_global_seed(project_cfg.get(\"seed\", 42))\n",
    "device_type = detect_device_type()\n",
    "LOGGER.info(\"Detected backend: %s\", device_type)\n",
    "\n",
    "if model_cfg.get(\"use_4bit\") and device_type != \"cuda\":\n",
    "    LOGGER.warning(\"4-bit quantization requires CUDA; disabling.\")\n",
    "    model_cfg[\"use_4bit\"] = False\n",
    "\n",
    "def _split_query(base: str, max_key: str) -> str:\n",
    "    max_samples = data_cfg.get(max_key)\n",
    "    return f\"{base}[:{int(max_samples)}]\" if max_samples else base\n",
    "\n",
    "train_query = _split_query(\"train\", \"max_train_samples\")\n",
    "eval_base = \"validation\"\n",
    "eval_query = _split_query(eval_base, \"max_eval_samples\")\n",
    "\n",
    "try:\n",
    "    raw_datasets = load_translation_dataset(\n",
    "        data_cfg[\"dataset_name\"],\n",
    "        data_cfg.get(\"dataset_config\"),\n",
    "        cache_dir=project_cfg.get(\"cache_dir\"),\n",
    "        split_overrides={\"train\": train_query, eval_base: eval_query},\n",
    "    )\n",
    "    eval_split = eval_base\n",
    "except ValueError:\n",
    "    eval_base = \"test\"\n",
    "    eval_query = _split_query(eval_base, \"max_eval_samples\")\n",
    "    raw_datasets = load_translation_dataset(\n",
    "        data_cfg[\"dataset_name\"],\n",
    "        data_cfg.get(\"dataset_config\"),\n",
    "        cache_dir=project_cfg.get(\"cache_dir\"),\n",
    "        split_overrides={\"train\": train_query, eval_base: eval_query},\n",
    "    )\n",
    "    eval_split = eval_base\n",
    "\n",
    "LOGGER.info(\"Loaded splits: train=%s, eval=%s\", train_query, eval_query)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_cfg[\"name\"], cache_dir=project_cfg.get(\"cache_dir\"))\n",
    "tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "tokenizer.src_lang = model_cfg[\"tokenizer_src_lang_code\"]\n",
    "tokenizer.tgt_lang = model_cfg[\"tokenizer_tgt_lang_code\"]\n",
    "\n",
    "preprocess_fn = build_preprocess_function(tokenizer, data_cfg)\n",
    "remove_columns = raw_datasets[\"train\"].column_names\n",
    "processed_datasets = raw_datasets.map(\n",
    "    preprocess_fn,\n",
    "    batched=True,\n",
    "    remove_columns=remove_columns,\n",
    "    desc=\"Tokenizing dataset\",\n",
    ")\n",
    "\n",
    "preview_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=None)\n",
    "train_loader = DataLoader(\n",
    "    processed_datasets[\"train\"],\n",
    "    batch_size=training_cfg[\"per_device_train_batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=preview_collator,\n",
    ")\n",
    "\n",
    "first_batch = next(iter(train_loader))\n",
    "print(\"Sample batch keys:\", first_batch.keys())\n",
    "print(\"input_ids shape:\", first_batch[\"input_ids\"].shape)\n",
    "print(\"labels shape:\", first_batch[\"labels\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344670c",
   "metadata": {},
   "source": [
    "## 3. æ„å»ºåŸºç¡€æ¨¡å‹ä¸ LoRA é€‚é…å™¨\n",
    "åŠ è½½ mBART ä¸»å¹²ã€å¯é€‰ 4-bit é‡åŒ–ï¼Œé…ç½® LoRA Rank/Alpha/Dropout å¹¶ä»…è®­ç»ƒç›®æ ‡æ¨¡å—ï¼Œæœ€åæ£€æŸ¥å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe779e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = get_dtype(model_cfg.get(\"torch_dtype\", \"float32\"))\n",
    "if device_type != \"cuda\" and dtype in (torch.float16, torch.bfloat16):\n",
    "    LOGGER.warning(\"%s backend detected; falling back to float32 weights.\", device_type)\n",
    "    dtype = torch.float32\n",
    "\n",
    "if device_type != \"cuda\":\n",
    "    for flag in (\"bf16\", \"fp16\", \"gradient_checkpointing\"):\n",
    "        if training_cfg.get(flag):\n",
    "            LOGGER.info(\"Disabling %s for %s backend.\", flag, device_type)\n",
    "            training_cfg[flag] = False\n",
    "    if training_cfg.get(\"dataloader_num_workers\", 0) > 0:\n",
    "        training_cfg[\"dataloader_num_workers\"] = 0\n",
    "    training_cfg[\"dataloader_pin_memory\"] = False\n",
    "\n",
    "model_kwargs: Dict[str, Any] = {\"cache_dir\": project_cfg.get(\"cache_dir\")}\n",
    "if model_cfg.get(\"use_4bit\"):\n",
    "    quant_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "    model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=quant_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    model_kwargs[\"device_map\"] = \"auto\"\n",
    "else:\n",
    "    model_kwargs[\"torch_dtype\"] = dtype\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_cfg[\"name\"], **model_kwargs)\n",
    "if model_cfg.get(\"use_4bit\"):\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "if hasattr(model.config, \"forced_bos_token_id\") and hasattr(tokenizer, \"lang_code_to_id\"):\n",
    "    model.config.forced_bos_token_id = tokenizer.lang_code_to_id[model_cfg[\"tokenizer_tgt_lang_code\"]]\n",
    "\n",
    "task_type = TaskType.SEQ_2_SEQ_LM if lora_cfg[\"task_type\"].lower() == \"seq2seq_lm\" else TaskType.SEQ_CLS\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_cfg[\"r\"],\n",
    "    lora_alpha=lora_cfg[\"alpha\"],\n",
    "    target_modules=lora_cfg[\"target_modules\"],\n",
    "    lora_dropout=lora_cfg[\"dropout\"],\n",
    "    bias=lora_cfg.get(\"bias\", \"none\"),\n",
    "    task_type=task_type,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "if training_cfg.get(\"gradient_checkpointing\"):\n",
    "    enable_gradient_checkpointing(model)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable params: {trainable_params/1e6:.2f}M / {total_params/1e6:.2f}M ({100*trainable_params/total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c98e46e",
   "metadata": {},
   "source": [
    "## 4. å®ç° `train_lora_mt` è®­ç»ƒå¾ªç¯\n",
    "æ­¤å¤„å¤ç°è„šæœ¬è®­ç»ƒé€»è¾‘ï¼šè‡ªå®šä¹‰å¤šä»»åŠ¡æŸå¤±èšåˆ $\\mathcal{L}_{total}=\\sum_i w_i\\mathcal{L}_i$ï¼Œé…ç½®ä¼˜åŒ–å™¨/è°ƒåº¦å™¨ã€æ¢¯åº¦ç´¯ç§¯ä¸æ–­ç‚¹æ¢å¤ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = {\"translation\": 1.0}  # å¯æŒ‰ä»»åŠ¡ç»´åº¦è°ƒæ•´\n",
    "\n",
    "\n",
    "class WeightedSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    \"\"\"ç®€å•ç¤ºä¾‹ï¼šæŒ‰ task_id èšåˆå¤šä»»åŠ¡æŸå¤±ï¼Œå¯æ‰©å±•ä¸ºçœŸå®å¤šä»»åŠ¡åœºæ™¯ã€‚\"\"\"\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        task_ids = inputs.pop(\"task_id\", None)\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        if task_ids is not None:\n",
    "            # task_id -> æƒé‡ï¼›æœ¬ä¾‹åªæœ‰ translation ä»»åŠ¡\n",
    "            unique_ids, counts = torch.unique(task_ids, return_counts=True)\n",
    "            weighted_loss = torch.zeros_like(loss)\n",
    "            for task, count in zip(unique_ids.tolist(), counts.tolist()):\n",
    "                weight = loss_weights.get(task, 1.0)\n",
    "                weighted_loss = weighted_loss + weight * loss / max(count, 1)\n",
    "            loss = weighted_loss\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "\n",
    "output_dir = Path(project_cfg[\"output_dir\"])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "training_args = Seq2SeqTrainingArguments(**prepare_training_arguments(project_cfg, training_cfg, output_dir))\n",
    "LOGGER.info(\"Training arguments: %s\", training_args)\n",
    "\n",
    "trainer = WeightedSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[eval_split],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "resume_checkpoint = project_cfg.get(\"resume_from_checkpoint\")\n",
    "train_result = trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "trainer.save_model()\n",
    "\n",
    "train_metrics = train_result.metrics\n",
    "train_metrics[\"train_samples\"] = len(processed_datasets[\"train\"])\n",
    "trainer.log_metrics(\"train\", train_metrics)\n",
    "trainer.save_metrics(\"train\", train_metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971f403",
   "metadata": {},
   "source": [
    "## 5. éªŒè¯ä¸æŒ‡æ ‡å¯è§†åŒ–\n",
    "å®ç° `evaluate()` è®¡ç®— BLEU/Perplexity/Accuracyï¼Œå¹¶æŠŠæ—¥å¿—å†™å…¥ pandasï¼Œå€ŸåŠ© matplotlib å¯è§†åŒ– loss ä¸å­¦ä¹ ç‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0d630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def evaluate_model(max_length: int | None = None, num_beams: int | None = None) -> Dict[str, float]:\n",
    "    eval_kwargs = {}\n",
    "    if max_length:\n",
    "        eval_kwargs[\"max_length\"] = max_length\n",
    "    if num_beams:\n",
    "        eval_kwargs[\"num_beams\"] = num_beams\n",
    "    eval_raw = trainer.evaluate(**eval_kwargs)\n",
    "\n",
    "    predicts = trainer.predict(processed_datasets[eval_split], **eval_kwargs)\n",
    "    decoded_preds = tokenizer.batch_decode(predicts.predictions, skip_special_tokens=True)\n",
    "    labels = np.where(predicts.label_ids != -100, predicts.label_ids, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    bleu = bleu_metric.compute(predictions=decoded_preds, references=[[lbl] for lbl in decoded_labels])\n",
    "    exact_match = np.mean([int(p == l) for p, l in zip(decoded_preds, decoded_labels)])\n",
    "    ppl = math.exp(min(eval_raw[\"eval_loss\"], 20))\n",
    "\n",
    "    metrics = {\n",
    "        **eval_raw,\n",
    "        \"bleu\": bleu[\"score\"],\n",
    "        \"exact_match\": float(exact_match),\n",
    "        \"perplexity\": float(ppl),\n",
    "    }\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "eval_metrics = evaluate_model(\n",
    "    max_length=training_cfg.get(\"generation_max_length\"),\n",
    "    num_beams=training_cfg.get(\"generation_num_beams\"),\n",
    ")\n",
    "print(\"Eval metrics:\", eval_metrics)\n",
    "\n",
    "# è®°å½•å­¦ä¹ ç‡ä¸æŸå¤±ï¼Œå¯è§†åŒ–\n",
    "log_history = pd.DataFrame(trainer.state.log_history)\n",
    "log_history = log_history.dropna(subset=[\"loss\"], how=\"all\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "log_history.plot(x=\"step\", y=\"loss\", ax=axes[0], title=\"Training loss\")\n",
    "if \"learning_rate\" in log_history:\n",
    "    log_history.plot(x=\"step\", y=\"learning_rate\", ax=axes[1], title=\"Learning rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc03c0",
   "metadata": {},
   "source": [
    "## 6. æ¨ç†æµ‹è¯•ä¸æƒé‡ä¿å­˜\n",
    "é€šè¿‡è‡ªå®šä¹‰ `generate_translation()` å¯¹æ ·ä¾‹æç¤ºåšæ¨ç†ï¼Œå¯é€‰å¯¹æ¯”æœª LoRA çš„ baselineï¼Œå¹¶åˆ†åˆ«ä¿å­˜ LoRA é€‚é…å™¨ä¸åˆå¹¶åçš„å…¨é‡æƒé‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49bbcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARE_BASELINE = False  # è‹¥ä¸º Trueï¼Œåˆ™é¢å¤–è½½å…¥æœª LoRA çš„ backbone è¿›è¡Œå¯¹æ¯”\n",
    "\n",
    "\n",
    "def generate_translation(text: str, model_to_use=None, tokenizer_to_use=None, max_new_tokens: int = 128) -> str:\n",
    "    model_to_use = model_to_use or trainer.model\n",
    "    tokenizer_to_use = tokenizer_to_use or tokenizer\n",
    "    prompt = data_cfg[\"prompt_template\"].format(\n",
    "        source_lang=data_cfg[\"source_language\"],\n",
    "        target_lang=data_cfg[\"target_language\"],\n",
    "        source_text=text,\n",
    "    )\n",
    "    inputs = tokenizer_to_use(prompt, return_tensors=\"pt\").to(model_to_use.device)\n",
    "    with torch.inference_mode():\n",
    "        generated = model_to_use.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=4)\n",
    "    return tokenizer_to_use.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "demo_text = \"The upcoming weekend weather looks great for hiking.\"\n",
    "print(\"LoRA output:\", generate_translation(demo_text))\n",
    "\n",
    "if COMPARE_BASELINE:\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_cfg[\"name\"],\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        cache_dir=project_cfg.get(\"cache_dir\"),\n",
    "    )\n",
    "    base_model.eval()\n",
    "    print(\"Baseline output:\", generate_translation(demo_text, model_to_use=base_model))\n",
    "\n",
    "# ä¿å­˜ LoRA é€‚é…å™¨\n",
    "lora_dir = output_dir / \"lora_adapter\"\n",
    "lora_dir.mkdir(exist_ok=True)\n",
    "trainer.model.save_pretrained(lora_dir)\n",
    "tokenizer.save_pretrained(lora_dir)\n",
    "print(\"LoRA adapter saved to\", lora_dir)\n",
    "\n",
    "# åˆå¹¶æƒé‡å¹¶å¯¼å‡ºï¼ˆç”¨äºçº¯æ¨ç†éƒ¨ç½²ï¼‰\n",
    "merged_dir = output_dir / \"merged_full_model\"\n",
    "merged_dir.mkdir(exist_ok=True)\n",
    "merged_model = trainer.model.merge_and_unload()\n",
    "merged_model.save_pretrained(merged_dir)\n",
    "tokenizer.save_pretrained(merged_dir)\n",
    "print(\"Merged model saved to\", merged_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
